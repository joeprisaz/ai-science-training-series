{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ConvNet\n",
    "\n",
    "In this section we'll build and apply a conv net to the mnist dataset.  The layers here are loosely based off of the ConvNext architecture.  Why?  Because we're getting into LLM's soon, and this ConvNet uses LLM features.  ConvNext is an update to the ResNet architecture that outperforms it.\n",
    "\n",
    "[ConvNext](https://arxiv.org/abs/2201.03545)\n",
    "\n",
    "The dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torchvision.transforms import v2\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=True, \n",
    "    num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=False, \n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch, (X, Y) = next(enumerate(train_dataloader))\n",
    "plt.imshow(X[0].cpu().permute((1,2,0))); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # CIFAR-10 is *color* images so 3 layers!\n",
    "    return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "val_dataloader = WrappedDataLoader(val_dataloader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size = stride,\n",
    "            stride = stride,\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "        \n",
    "        \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=in_channels, \n",
    "                                     groups=in_channels,\n",
    "                                     kernel_size=[7,7],\n",
    "                                     padding='same' )\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=4*in_channels,\n",
    "                                     kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # This is a downsampling convolution that will produce patches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                                    out_channels=n_initial_filters,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1)\n",
    "        \n",
    "        current_shape = [32, 32]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters,*current_shape])\n",
    "        # self.norm1 = WrappedLayerNorm()\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters, \n",
    "                    out_channels=2*current_n_filters,\n",
    "                    shape = current_shape,\n",
    "                    )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2*current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [ cs // 2 for cs in current_shape]\n",
    "            \n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, 10)\n",
    "        )\n",
    "        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n",
    "        # # This brings it down to one channel / class\n",
    "        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n",
    "        #                                   kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.bottleneck(x)\n",
    "\n",
    "        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n",
    "        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Classifier                               [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         256\n",
      "├─LayerNorm: 1-2                         [128, 64, 32, 32]         131,072\n",
      "├─Sequential: 1-3                        [128, 512, 4, 4]          --\n",
      "│    └─ConvNextBlock: 2-1                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 32, 32]         3,200\n",
      "│    │    └─LayerNorm: 3-2               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-3                  [128, 256, 32, 32]        16,640\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 32, 32]         16,448\n",
      "│    └─ConvNextBlock: 2-2                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-5                  [128, 64, 32, 32]         3,200\n",
      "│    │    └─LayerNorm: 3-6               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-7                  [128, 256, 32, 32]        16,640\n",
      "│    │    └─Conv2d: 3-8                  [128, 64, 32, 32]         16,448\n",
      "│    └─Downsampler: 2-3                  [128, 128, 16, 16]        --\n",
      "│    │    └─LayerNorm: 3-9               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-10                 [128, 128, 16, 16]        32,896\n",
      "│    └─ConvNextBlock: 2-4                [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-11                 [128, 128, 16, 16]        6,400\n",
      "│    │    └─LayerNorm: 3-12              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-13                 [128, 512, 16, 16]        66,048\n",
      "│    │    └─Conv2d: 3-14                 [128, 128, 16, 16]        65,664\n",
      "│    └─ConvNextBlock: 2-5                [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-15                 [128, 128, 16, 16]        6,400\n",
      "│    │    └─LayerNorm: 3-16              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-17                 [128, 512, 16, 16]        66,048\n",
      "│    │    └─Conv2d: 3-18                 [128, 128, 16, 16]        65,664\n",
      "│    └─Downsampler: 2-6                  [128, 256, 8, 8]          --\n",
      "│    │    └─LayerNorm: 3-19              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-20                 [128, 256, 8, 8]          131,328\n",
      "│    └─ConvNextBlock: 2-7                [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-21                 [128, 256, 8, 8]          12,800\n",
      "│    │    └─LayerNorm: 3-22              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-23                 [128, 1024, 8, 8]         263,168\n",
      "│    │    └─Conv2d: 3-24                 [128, 256, 8, 8]          262,400\n",
      "│    └─ConvNextBlock: 2-8                [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-25                 [128, 256, 8, 8]          12,800\n",
      "│    │    └─LayerNorm: 3-26              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-27                 [128, 1024, 8, 8]         263,168\n",
      "│    │    └─Conv2d: 3-28                 [128, 256, 8, 8]          262,400\n",
      "│    └─Downsampler: 2-9                  [128, 512, 4, 4]          --\n",
      "│    │    └─LayerNorm: 3-29              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-30                 [128, 512, 4, 4]          524,800\n",
      "│    └─ConvNextBlock: 2-10               [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-31                 [128, 512, 4, 4]          25,600\n",
      "│    │    └─LayerNorm: 3-32              [128, 512, 4, 4]          16,384\n",
      "│    │    └─Conv2d: 3-33                 [128, 2048, 4, 4]         1,050,624\n",
      "│    │    └─Conv2d: 3-34                 [128, 512, 4, 4]          1,049,088\n",
      "│    └─ConvNextBlock: 2-11               [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-35                 [128, 512, 4, 4]          25,600\n",
      "│    │    └─LayerNorm: 3-36              [128, 512, 4, 4]          16,384\n",
      "│    │    └─Conv2d: 3-37                 [128, 2048, 4, 4]         1,050,624\n",
      "│    │    └─Conv2d: 3-38                 [128, 512, 4, 4]          1,049,088\n",
      "├─Sequential: 1-4                        [128, 10]                 --\n",
      "│    └─Flatten: 2-12                     [128, 512]                --\n",
      "│    └─LayerNorm: 2-13                   [128, 512]                1,024\n",
      "│    └─Linear: 2-14                      [128, 10]                 5,130\n",
      "==========================================================================================\n",
      "Total params: 7,223,562\n",
      "Trainable params: 7,223,562\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 39.46\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 2072.52\n",
      "Params size (MB): 28.89\n",
      "Estimated Total Size (MB): 2102.99\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(64, 4, 2)\n",
    "\n",
    "# model.cuda() # I do not have a GPU\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "            \n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        progress_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0cba3d227045e399e05ace3397adc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_bar:\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# checking on the training loss and accuracy once per epoch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidate (train) Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_eval:\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer, progress_bar)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# backward pass calculates gradients\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 130\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Apply the main chunk of the network:\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Normalize and readout:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mavg_pool2d(x, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mConvNextBlock.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# The non-linear activation layer:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[0;32m---> 65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# This makes it a residual network:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m inputs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "        \n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "        acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "\n",
    "        print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "    \n",
    "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1:\n",
    "\n",
    "In this notebook, we've learned about some basic convolutional networks and trained one on CIFAR-10 images.  It did ... OK.  There is significant overfitting of this model.  There are some ways to address that, but we didn't have time to get into that in this session.\n",
    "\n",
    "Meanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture.  Change one or more of the following:\n",
    "- The number of convolutions between downsampling\n",
    "- The number of filters in each layer\n",
    "- The initial \"patchify\" layer\n",
    "- Another hyper-parameter of your choosing\n",
    "\n",
    "\n",
    "And compare your final validation accuracy to the accuracy shown here.  Can you beat the validation accuracy shown?\n",
    "\n",
    "For full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets' performance (loss and accuracy) for all the epochs you train.  You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
